{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D multivariate ENSO index (MEI) using ECMWF ERA-20C data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore the ERA-20C dataset provided by ECMWF. In particular, we will reproduce the multivariate ENSO index (MEI) that was originally published by [Wolter and Timlin: Monitoring ENSO in COADS with a seasonally adjusted principal component index, (1993)](https://psl.noaa.gov/enso/mei.old/WT1.pdf) and reproduced in [Schmidt et al. Spectral Empirical Orthogonal Function Analysis of Weather and Climate Data, 2019](https://flowphysics.ucsd.edu/wp-content/papercite-data/pdf/schmidtetal_2019_mwr.pdf). \n",
    "Multivariate indices, like the MEI index are typically used to reveal interplay among different variables. The MEI index is composed of 6 variables: \n",
    "- Mean sea level pressure (MSL)\n",
    "- Zonal component of the surface wind (U10)\n",
    "- Meridional component of the surface wind (V10)\n",
    "- Sea surface temperature (SST)\n",
    "- 2-meter temperature (T2M), and \n",
    "- Total cloud cover (TCC).\n",
    "\n",
    "We will download the monthly averages of the above variables from 1900 to 2010. \n",
    "\n",
    "As originally done in the original work by *Wolter and Timlin*, we normalize the data associated to each variable by its total variance. We will see how the use of PySPOD helps identifying spatio-temporal coherent structures associated to the 6 variables considered, and make some considerations on the possible interplay among them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to use `PySPOD` for investigating the multivariate ENSO index (MEI), is to download the relevant data. This step can require a few hours, depending on your internet connection and on how busy ECMWF server is. Also, please note that the amount of data that needs to be downloaded is approximately 443MB. \n",
    "\n",
    "In order to download the required data from the ECMWF, you should create an account and follow the instructions here: https://confluence.ecmwf.int/display/WEBAPI/Access+ECMWF+Public+Datasets\n",
    "\n",
    "Once you have an account to access ECMWF data, you can simply run:\n",
    "`python3 E20C_MONTHLYMEAN00_1900_2010_MEI.py`\n",
    "\n",
    "This should download a netCDF file called `E20C_MONTHLYMEAN00_1900_2010_MEI.nc` in the current directory `tutorials/climate/ERA20C_MEI_2D/`. Please, make sure that this is the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and configuring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have gained access to ECMWF datasets and downloaded the required file `E20C_MONTHLYMEAN00_1900_2010_MEI.nc`, we can proceed to use `PySPOD` to identify coherent structures in the data. The first step from this perspective is to import the required libraries, including the custom libraries \n",
    "- `from pyspod.spod.standard  import Standard  as spod_standard`\n",
    "- `from pyspod.spod.streaming import Streaming as spod_streaming`\n",
    "\n",
    "that include two different implementations of the SPOD algorithm, the first being a standard algorithm, and the second being a streaming algorithm, that requires little amount of memory (both storage and RAM) but runs typically slower than the standard algorithm.\n",
    "\n",
    "Note that we also import the custom library \n",
    "- `import library.weights as weights`\n",
    "\n",
    "that implements the weight matrix for data defined on a sphere (e.g. the atmospheric data we are using). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import numpy  as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Current path\n",
    "CWD = os.getcwd()\n",
    "\n",
    "# Import library specific modules\n",
    "sys.path.insert(0, \"../../../\")\n",
    "from pyspod.spod.standard  import Standard  as spod_standard\n",
    "from pyspod.spod.streaming import Streaming as spod_streaming\n",
    "import pyspod.utils.weights  as utils_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to load the data from the netCDF file and inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and load data \n",
    "file = os.path.join(CWD,'ERA5_monthly_1940_2024.nc')\n",
    "ds = xr.open_dataset(file)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the netCDF file contains **3 coordinates**:\n",
    "- longitude,\n",
    "- latitude,  \n",
    "- time, \n",
    "\n",
    "along with **6 variables**:\n",
    "\n",
    "- sst (sea surface temperature),\n",
    "- msl (mean seal level pressure),\n",
    "- tcc (total cloud cover),\n",
    "- u10 (horizontal velocity; u-component),\n",
    "- v10 (horizontal velocity; v-component),\n",
    "- t2m (2-meters temperature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load `time`, and the two spatial coordinates `longitude` and `latitude`, and we store them into three different arrays, `t`, `x1` and `x2`, respectively. Let's look at their dimensions to see the number of time snapshots, and the length of longitude and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract time, longitude and latitude\n",
    "t = np.array(ds['valid_time'])\n",
    "x1 = np.array(ds['longitude'])\n",
    "x2 = np.array(ds['latitude'])\n",
    "nt = t.shape[0]\n",
    "print('shape of t (time): ', t.shape)\n",
    "print('shape of x1 (longitude): ', x1.shape)\n",
    "print('shape of x2 (latitude) : ', x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to load the data in a format that is suitable to the `PySPOD` library, that is\n",
    "- first dimension must correspond to the number of time snapshots (1332 in our case)\n",
    "- last dimension should corresponds to the number of variables (6 in our case)\n",
    "- the remaining dimensions corresponds to the spatial dimensions (240, and 121 in our case, that correspond to longitude and latitude).\n",
    "\n",
    "Let's load and reshuffle it, in order to meet this requirement, and check its dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the variables we want to use for the analysis\n",
    "# (we select all the variables present) and load them in RAM\n",
    "variables = ['sst', 'msl', 'tcc', 'u10', 'v10', 't2m']\n",
    "X = np.empty([t.shape[0], x1.shape[0], x2.shape[0], len(variables)])\n",
    "for i,var in enumerate(variables):\n",
    "    X[...,i] = np.einsum('ijk->ikj', np.array(ds[var]))\n",
    "    X[...,i] = np.nan_to_num(X[...,i])\n",
    "print('shape of data matrix X: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that: \n",
    "- the **first dimension is 1332**, and corresponds to the **number of time snapshots**; \n",
    "- the **second is 240**, and corresponds to the **number of longitude points**;\n",
    "- the **third is 121**, and corresponds to the **number of latitude points**;\n",
    "- the **fourth is 6**, and corresponds to the **number of variables**.\n",
    "\n",
    "Indeed, we use as first dimension the number of time snapshots, as last dimension the number of variables and as middle dimensions, the spatial dimensions, that in our case are two and correspond to longitude and latitude.\n",
    "\n",
    "<font color='green'> **NOTE I : we used `np.nan_to_num` to set possible NaN (not-a-number) to zero. The amount of NaN in this case is relatively small, and setting them to zero is a feasible approximation. For your specific application you might want to apply a different replacement for NaN (if present).**</font>\n",
    "\n",
    "<font color='green'> **NOTE II: we also used `np.einsum` in order to match the axes dimensions to the dimensions of the data, i.e. to obtain a matrix shape $1332 \\times 240 \\times 121$.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting required and optional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data is in a shape suitable to the `PySPOD` library, we define the **required** and **optional parameters**. In particular, we define a dictionary of parameters, that will be passed to the constructor of `PySPOD`. \n",
    "\n",
    "The required parameters are as follows:\n",
    "- `time_step`: time-sampling of the data (for now this must be constant)\n",
    "- `n_snapshots`: number of time snapshots\n",
    "- `n_space_dims`: number of spatial dimensions\n",
    "- `n_variables`: number of variables\n",
    "- `n_dft`: length of FFT blocks\n",
    "\n",
    "The optional parameters are as follows:\n",
    "- `overlap`: dimension of the overlap region between adjacent blocks in percentage (0 to 100)\n",
    "- `mean_type`: type of mean to be subtracted from the data (`longtime`, `blockwise` or `zero`)\n",
    "- `normalize_weights`: weights normalization by data variance\n",
    "- `normalize_data`: normalize data by variance\n",
    "- `n_modes_save`: number of modes to be saved\n",
    "- `conf_level`: calculate confidence level of modes\n",
    "- `reuse_blocks`: whether to attempt reusing FFT blocks previously computed (if found)\n",
    "- `savefft`: save FFT blocks to reuse them in the future (to save time)\n",
    "- `savedir`: where to save the data\n",
    "\n",
    "\n",
    "<font color='green'> **Note: we used the built-in module `utils_weights`, and in particular the function `utils_weights.geo_trapz_2d` to construct the weights for our problem on the sphere. This function builds weights according to lon-lat grids on the sphere and can be generally used for  two-dimensional atmospheric data. You can implement your own weights into the `custom` function inside `utils_weights.py` or by readily providing the weights as a numpy.ndarray. The weights must have a shape equal to the data spatial dimensions $\\times$ number of variables, that is:**\n",
    "    \n",
    "$$\\textsf{weights dim} = \\textsf{number of spatial dimensions} \\times \\textsf{number of variables}$$. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define required and optional parameters\n",
    "params = dict()\n",
    "\n",
    "# -- required parameters\n",
    "params['time_step'   ] = 720              # data time-sampling\n",
    "params['n_snapshots' ] = len(t)           # number of time snapshots (we consider all data)\n",
    "params['n_space_dims'] = 2                # number of spatial dimensions (longitude and latitude)\n",
    "params['n_variables' ] = len(variables)   # number of variables\n",
    "params['n_dft'       ] = np.ceil(12 * 5)  # length of FFT blocks (100 time-snapshots)\n",
    "\n",
    "# -- optional parameters\n",
    "params['overlap'          ] = 0             # dimension block overlap region\n",
    "params['mean_type'        ] = 'blockwise'   # type of mean to subtract to the data\n",
    "params['normalize_weights'] = True          # normalization of weights by data variance\n",
    "params['normalize_data'   ] = False         # normalize data by data variance\n",
    "params['n_modes_save'     ] = 3             # modes to be saved\n",
    "params['conf_level'       ] = 0.95          # calculate confidence level\n",
    "params['reuse_blocks'     ] = False         # whether to reuse blocks if present\n",
    "params['savefft'          ] = False         # save FFT blocks to reuse them in the future (saves time)\n",
    "params['savedir'          ] = os.path.join(CWD, 'results', Path(file).stem) # folder where to save results\n",
    "\n",
    "# Set weights\n",
    "weights = utils_weights.geo_trapz_2D(\n",
    "\tx1_dim=x2.shape[0], x2_dim=x1.shape[0],\n",
    "\tn_vars=len(variables), R=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this tutorial we normalize the weights by data variance by setting `params['normalize_weights'] = True`. This step is performed as in (Wolter and Timlin, 1993) and (Schmidt et al., 2019). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the SPOD analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the data and defined the required and optional parameters, we can perform the analysis. This step is accomplished by calling the `PySPOD` constructor, `spod_standard(params=params, weights=weights)` and the `fit` method, `SPOD_analysis.fit(data_list=X)`. \n",
    "\n",
    "The `PySPOD` constructor takes the parameters `params`, and `weights`.\n",
    "\n",
    "The method `fit` takes as inputs `data_list`, that can either be a `numpy.ndarray` containing the data or a list of `xarray.DataArray`s.\n",
    "  \n",
    "The `fit` method returns a `PySPOD` object containg the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform SPOD analysis using the standard module\n",
    "SPOD_analysis = spod_standard(\n",
    "\tparams=params,\n",
    "\tweights=weights)\n",
    "\n",
    "# Fit SPOD\n",
    "spod = SPOD_analysis.fit(data_list=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing and visualizing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are stored in a `PySPOD` object that is composed by: \n",
    "- a set of eigenvalues per each frequency computed, and \n",
    "- a set of modes, per each frequency computed. \n",
    "\n",
    "In order to visualize them, we can use the built-in plotting functionalities of `PySPOD`.\n",
    "We first select the frequency (equivalently period T_approx), that we want to investigate, and identify the nearest frequency in the results by using the built-in functions `find_nearest_freq`, and `get_modes_at_freq`, that are part of the `postproc` module, and can be directly called from the `PySPOD` object returned once the `fit` method has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "T_approx = 876 # approximate period (in days)\n",
    "freq_found, freq_idx = spod.find_nearest_freq(freq_req=1/T_approx, freq=spod.freq)\n",
    "modes_at_freq = spod.get_modes_at_freq(freq_idx=freq_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the **eigenvalues in the complex plane**, using the built-in function `plot_eigs`, that is part of the `postproc` module. We note that the eigenvalues are all real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spod.plot_eigs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the **eigenvalues as a function of frequency and period** (note that we multiply the frequency by 24, that is the number of hours in a day, to obtain a period for the x-axis in days). Again, we can see how thorough the `PySPOD` object returned after the computation we can access the frequency array (`spod.freq`) along with the plotting methods `spod.plot_eigs_vs_frequency` and `spod.plot_eigs_vs_period`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = spod.freq*24\n",
    "spod.plot_eigs_vs_frequency(freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spod.plot_eigs_vs_period(freq=freq, xticks=[1, 7, 30, 365, 1825])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the **modes** that were computed by the SPOD algorithm via the built-in `plot_2d_modes_at_frequency` method, that can again be accessed via the `PySPOD` object returned after the computation. To this method, we pass the frequency of the modes we are interested in. This corresponds to the frequency associated to the T_approx of 876 days that we requested, and stored in the variable `freq_found` that we calculated above. Note that we also pass the `vars_idx` corresponding to the variable we are interested in, modes_idx corresponding to the modes we are interested in, as well as `x1`, and `x2`, that correspond to longitude and latitude. We finally pass a parameter `coastlines='centred'`, that overlays a world map on top of our data. Note that we **shift longitude of 180**, as we use coastlines that goes from -180, to 180, while our data was provided from 0 to 360. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spod.plot_2d_modes_at_frequency(\n",
    "    freq_req=freq_found, \n",
    "    freq=freq, \n",
    "    x1=x1-180, \n",
    "    x2=x2, \n",
    "    coastlines='centred', \n",
    "    modes_idx=[0,1], \n",
    "    vars_idx=[1,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also plot the **original data** by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = spod.get_data(X)\n",
    "\n",
    "spod.plot_2d_data(\n",
    "    data,\n",
    "    x1=x1-180, \n",
    "    x2=x2, \n",
    "    coastlines='centred', \n",
    "    vars_idx=[5], \n",
    "    time_idx=[0,100,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with a video of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spod.generate_2d_data_video(\n",
    "    data,\n",
    "    x1=x1-180, \n",
    "    x2=x2, \n",
    "    coastlines='centred', \n",
    "    sampling=20,\n",
    "    vars_idx=[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are stored in the results folder defined in the parameter you specified under `params['savedir']`. You can load the results for both modes and eigenvalues, and use any other postprocessing tool that is more suitable to your application. The files are stored in `numpy` binary format `.npy`. There exist several tools to convert them in `netCDF`, `MATLAB` and several other formats that can be better suited to your specific post-processing pipeline.\n",
    "\n",
    "This tutorial was intended to help you setup your own multivariate case. You can play with the parameters we explored above to gain more insights into the capabilities of the library. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
